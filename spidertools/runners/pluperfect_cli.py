import argparse
import os
import yaml
import json
from spidertools.utils.analysis_repo import AnalysisRepo
from spidertools.tools.tacoco import TacocoRunner
from spidertools.tools.history import MethodParserRunner
from spidertools.process_data.coverage_json import coverage_json
from spidertools.storage.table_handlers import ProjectTableHandler, CommitTableHandler, MethodCoverageHandler

# By default the database is only created in memory
DB_PATH = ":memory:"

def parse_arguments():
    """
    Parse arguments to run Tacoco
    """
    parser = argparse.ArgumentParser(description='Run Tacoco')

    # TODO: Add option to switch between number of releases and number of commits
    parser.add_argument('project_url', type=str, help="Absolute path or url to system-under-test.")
    parser.add_argument('--output_path', type=str, help="absolute path to output directory")
    parser.add_argument('--current', action='store_true', help="Run the analysis on current version of the code")
    parser.add_argument('--config', type=str, help="absolute path to tacoco")

    return parser.parse_args()

def start(project_url, output_path, tacoco_path, history_slider_path, single_run):
    global DB_PATH

    project_handler = ProjectTableHandler(DB_PATH)
    commit_handler = CommitTableHandler(DB_PATH)
    coverage_handler = MethodCoverageHandler(DB_PATH)

    with AnalysisRepo(project_url) as repo:
        # Add project
        if (project_entry := project_handler.get_project_id(repo.get_project_name())) is None:
            project_id: int = project_handler.add_project(repo.get_project_name())
        else:
            project_id = project_entry['project_id']
        
        # Analysis tools
        tacoco_runner = TacocoRunner(repo, output_path, tacoco_path)
        parser_runner = MethodParserRunner(repo, output_path, history_slider_path)

        # Get all commits we want to run the analysis on.
        commits = []
        if single_run:
            commits = [repo.get_current_commit()]
        else:
            commits = repo.iterate_tagged_commits(5)

        for commit in commits:    
            #  Check if database already has an entry for the commit, if so, skip.
            commit_entry = commit_handler.get_commit_id(project_id, commit)
            if commit_entry is not None:
                print(f'[Warning] commit ID already exists: {commit}')
                continue
            else:
                print(f'[Info] commit ID does not exist yet: {commit}')

            output = _analysis(repo, tacoco_runner, parser_runner, output_path)

            if output:
                # Get the commit ID from the database
                commit_sha = repo.get_current_commit()
                commit_id = commit_handler.add_commit(project_id, commit)

                # Store coverage data in database.
                coverage_handler.add_project_coverage(project_id, commit_id, output["methods"]["production"], output["methods"]["test"])
            else:
                print("Error: no or incorrect output generated by the analysis...")

def _analysis(repo, tacoco_runner, parser_runner, output_path):
    # Before each analysis remove all generated files of previous run
    repo.clean()

    # Start analysis
    build_output = tacoco_runner.build()

    if build_output == 1:
        print("[ERROR] build failure...")
    else:
        parser_runner.run()
        tacoco_runner.run()

        # Combine data
        commit_sha = repo.get_current_commit()
        commit_id = commit_sha
        project_output_path = f"{output_path}{os.path.sep}{repo.get_project_name()}{os.path.sep}"
        output = coverage_json(
            f"{project_output_path}methods-{commit_sha}.json",
            f"{project_output_path}{commit_sha}-cov-matrix.json",
            commit_id
        )

        return output

def main():
    print("Start analysis...")
    global DB_PATH

    arguments = parse_arguments()
    project_url = arguments.project_url
    config_path = arguments.config
    
    if config_path is not None:
        with open(config_path) as config_file:
            config = yaml.safe_load(config_file.read())

        analysis_config = config["analysis"]
        output_path = f"{os.getcwd()}{os.path.sep}"
        DB_PATH = analysis_config["DB_LOCATION"]
        if analysis_config["OUTPUT_DIR"]:
            output_path = analysis_config["OUTPUT_DIR"]
        tacoco_path = analysis_config["TACOCO_HOME"]
        history_slider_path = analysis_config["HISTORY_SLICER_HOME"]

    start(project_url, output_path, tacoco_path, history_slider_path, single_run=arguments.current)
